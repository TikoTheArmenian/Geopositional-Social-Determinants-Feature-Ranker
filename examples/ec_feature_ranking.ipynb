{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports / Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/98/46/f6a79f944d5c7763a9bc13b2aa6ac72daf43a6551f5fb03bccf0a9c2fec1/transformers-4.33.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/119.9 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 112.6/119.9 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 119.9/119.9 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting ankh\n",
      "  Obtaining dependency information for ankh from https://files.pythonhosted.org/packages/74/a7/ea313596f57c14890f1985ae7a7554698571c239108fa799b352a34441b7/ankh-1.10.0-py3-none-any.whl.metadata\n",
      "  Downloading ankh-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/2d/5e/9213ea10ac473e2437dc2cb17323ddc0999997e2713d6a0b683b10773994/pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "Collecting scipy\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/06/15/e73734f9170b66c6a84a0bd7e03586e87e77404e2eb8e34749fc49fa43f7/scipy-1.11.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.1/59.1 kB ? eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Obtaining dependency information for numpy>=1.17 from https://files.pythonhosted.org/packages/93/fd/3f826c6d15d3bdcf65b8031e4835c52b7d9c45add25efa2314b53850e1a2/numpy-1.26.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.1/61.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\logan hallee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/de/cd/d80c9e284ae6c1b2172dacf0651d25b78ee1f7efbc12d74ea7b87c766263/regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.7/3.5 MB 21.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.9/3.5 MB 24.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.5/3.5 MB 27.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 24.5 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/f3/c9/6f14c11265ecc9ff06fddae423eee40061c3841a1e44ca1e7b1bb8ce3b48/safetensors-0.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting biopython<2.0,>=1.80 (from ankh)\n",
      "  Downloading biopython-1.81-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------  2.7/2.7 MB 57.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.7/2.7 MB 57.7 MB/s eta 0:00:00\n",
      "Collecting sentencepiece<0.2.0,>=0.1.97 (from ankh)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 60.5 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/0e/2b/328c405ad7897db16f04c4998a3d514e5c311053a8dbf9cf575e4b3ff549/pyarrow-13.0.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/46/14/0302669d5d983ce23dc3870f4f2b16ab1d757a1d7e54a5cfe7a5df37f8e2/xxhash-3.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/21/4e/452858698e53ddf06ea137eac268db535c9605394c27236f9986168dd82f/aiohttp-3.8.5-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\logan hallee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ---------------------------------------- 0.0/341.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 341.8/341.8 kB 20.7 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for charset-normalizer<4.0,>=2.0 from https://files.pythonhosted.org/packages/91/6e/db0e545302bf93b6dbbdc496dd192c7f8e8c3bb1584acba069256d8b51d4/charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl.metadata (31 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/39/16/72d9ccd30815d0b37218348f053be37bc3d14288ac192a794a39990ac28e/frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.15.1->transformers)\n",
      "  Obtaining dependency information for typing-extensions>=3.7.4.3 from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\logan hallee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/37/dc/399e63f5d1d96bb643404ee830657f4dfcf8503f5ba8fa3c6d465d0c57fe/urllib3-2.0.5-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\logan hallee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 94.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 122.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 82.0 MB/s eta 0:00:00\n",
      "Using cached ankh-1.10.0-py3-none-any.whl (31 kB)\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "   ---------------------------------------- 0.0/519.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 519.6/519.6 kB 31.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.1.1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.2/10.6 MB 168.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 81.8 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "   ---------------------------------------- 0.0/44.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 5.2/44.0 MB 167.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.7/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 16.2/44.0 MB 129.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.7/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.1/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.1/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.0/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.1/44.0 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.0 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.0/44.0 MB 43.7 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.8.5-cp311-cp311-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 320.6/320.6 kB ? eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.0/295.0 kB ? eta 0:00:00\n",
      "Downloading numpy-1.26.0-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 5.1/15.8 MB 108.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.5/15.8 MB 131.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 73.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow-13.0.0-cp311-cp311-win_amd64.whl (24.3 MB)\n",
      "   ---------------------------------------- 0.0/24.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 3.5/24.3 MB 113.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.1/24.3 MB 117.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.8/24.3 MB 131.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.2/24.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.6/24.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.3 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.3/24.3 MB 54.4 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ---------------------------------------- 0.0/502.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 502.5/502.5 kB ? eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.8.8-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.3/268.3 kB ? eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.3.3-cp311-cp311-win_amd64.whl (266 kB)\n",
      "   ---------------------------------------- 0.0/266.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 266.1/266.1 kB ? eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB ? eta 0:00:00\n",
      "Downloading xxhash-3.3.0-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 158.3/158.3 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.6/96.6 kB ? eta 0:00:00\n",
      "Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "   ---------------------------------------- 0.0/123.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 123.8/123.8 kB ? eta 0:00:00\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 163.8/163.8 kB 10.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, regex, pyyaml, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, scipy, requests, pyarrow, pandas, multiprocess, biopython, aiosignal, huggingface-hub, aiohttp, transformers, datasets, ankh\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 ankh-1.10.0 async-timeout-4.0.3 attrs-23.1.0 biopython-1.81 certifi-2023.7.22 charset-normalizer-3.2.0 datasets-2.14.5 dill-0.3.7 filelock-3.12.4 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.17.3 idna-3.4 multidict-6.0.4 multiprocess-0.70.15 numpy-1.26.0 pandas-2.1.1 pyarrow-13.0.0 pytz-2023.3.post1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 safetensors-0.3.3 scipy-1.11.2 sentencepiece-0.1.99 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.33.3 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.0.5 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "pip install transformers xgboost datasets huggingface_hub ankh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6a63c3f3d14ae29b0b2c0625d315de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import ast\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/627 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81d53f5715fe4ca990ca18b7fb17a51c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e39478225bf4c79b479059322252bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/219M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f4996da9614801a9a4cfc39d229880"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8f68a3d39f2455fa9852979e9568a90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/530876 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b46a1cf2d46a4e0c821424910958341c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['Entry', 'EC number', 'Sequence', '1st', 'Class', 'group'],\n    num_rows: 530876\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('lhallee/ec_feature_ranking', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_paths = ['facebook/esm2_t6_8M_UR50D', 'facebook/esm2_t12_35M_UR50D', 'Rostlab/prot_bert',\n",
    "                'facebook/esm2_t30_150M_UR50D', 'facebook/esm2_t33_650M_UR50D',\n",
    "                'facebook/esm2_t36_3B_UR50D', 'facebook/esm2_t48_15B_UR50D']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Check if GPU is available\n",
    "\n",
    "for model_path in model_paths:\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModel.from_pretrained(model_path).to(device)  # Send the model to the GPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Initialize an empty list to store the embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over the sequences\n",
    "    for seq in tqdm(seqs, desc=model_path):\n",
    "        # Tokenize the sequence and get the model's output\n",
    "        inputs = tokenizer(seq, return_tensors='pt').input_ids.to(device)  # Send the inputs to the GPU\n",
    "        outputs = model(inputs)\n",
    "        # Get the last hidden state and compute the mean\n",
    "        embedding = outputs.last_hidden_state.mean(dim=0).cpu().detach().numpy()\n",
    "        # Append the mean embedding to the list\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Add the embeddings as a new column in the dataset\n",
    "    dataset['train'][model_path] = embeddings\n",
    "    dataset.push_to_hub('lhallee/ec_feature_ranking')\n",
    "\n",
    "    # Reset GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def hyper_param_search(df, targets, cv, model_params, num_runs):\n",
    "    X = np.array(df)\n",
    "    total_scores = pd.DataFrame()  # empty DataFrame to store scores from all iterations\n",
    "\n",
    "    for target_col in tqdm(targets.columns):\n",
    "        y = np.array(targets[target_col])\n",
    "        scores = []  # Score list for cv\n",
    "        # Randomized search CV on all models above\n",
    "        for model_name, mp in model_params.items():\n",
    "            clf = RandomizedSearchCV(mp['model'], mp['params'], n_iter=num_runs, cv=cv, return_train_score=False, random_state=42)\n",
    "            clf.fit(X, y)\n",
    "            scores.append({\n",
    "                'model': model_name,\n",
    "                'best_score': clf.best_score_,\n",
    "                'best_params': clf.best_params_,\n",
    "                'target_col': target_col  # adding target column name as an additional feature\n",
    "            })\n",
    "\n",
    "        cv_df = pd.DataFrame(scores, columns=['model','best_score','best_params', 'target_col'])  # create dataframe with each iteration's scores\n",
    "        total_scores = pd.concat([total_scores, cv_df])  # concatenate the new scores with the total scores\n",
    "\n",
    "    return total_scores.reset_index(drop=True)  # reset the index for the final DataFrame\n",
    "\n",
    "\n",
    "def lasso_ranking(X, y, step_a):\n",
    "    # Initialize\n",
    "    alpha = 0\n",
    "    last_coef = np.ones(len(X.columns))\n",
    "    ranking = pd.DataFrame(columns=['Lasso order', 'Coef'])\n",
    "    dropped_features = set()  # This set will store the features that have already been dropped\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # While loop until all coefficients are 0\n",
    "    while np.any(last_coef != 0):\n",
    "        las = Lasso(alpha)\n",
    "        las.fit(X_scaled, y)\n",
    "        coef = las.coef_\n",
    "\n",
    "        # Check which coefficients have just become 0\n",
    "        just_zeroed = (last_coef != 0) & (coef == 0)\n",
    "        zeroed_features = list(X.columns[just_zeroed])\n",
    "\n",
    "        # Update ranking\n",
    "        for feature in zeroed_features:\n",
    "            # Only add feature to ranking if it hasn't already been dropped\n",
    "            if feature not in dropped_features:\n",
    "                ranking = ranking.append({'Lasso order': feature, 'Coef': alpha,}, ignore_index=True)\n",
    "                dropped_features.add(feature)\n",
    "\n",
    "        # Update last_coef\n",
    "        last_coef = coef\n",
    "\n",
    "        # Increase alpha\n",
    "        alpha += step_a\n",
    "\n",
    "    # Sort ranking in descending order of alpha (ascending order of importance)\n",
    "    ranking = ranking.sort_values(by='Coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return ranking\n",
    "\n",
    "\n",
    "\n",
    "def ranking(df, targets, hypers_rf, hypers_xb, alpha):\n",
    "    X = np.array(df)\n",
    "    total_ranking = pd.DataFrame()\n",
    "\n",
    "    for target_col in targets.columns:\n",
    "        y = np.array(targets[target_col])\n",
    "        rf_hyper = hypers_rf[hypers_rf['target_col'] ==\n",
    "                             target_col].reset_index(drop=True)['best_params'].apply(ast.literal_eval)[0]\n",
    "        xb_hyper = hypers_xb[hypers_xb['target_col'] ==\n",
    "                             target_col].reset_index(drop=True)['best_params'].apply(ast.literal_eval)[0]\n",
    "        lasso_ranks = lasso_ranking(df, targets[target_col], alpha)\n",
    "        # Feature importance from RandomForest\n",
    "        rf = RandomForestClassifier(**rf_hyper)\n",
    "        rf.fit(X, y)\n",
    "        rf_importances = pd.DataFrame({'RF order':df.columns, 'Coef':rf.feature_importances_}).sort_values(by='Coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Feature importance from XGBoost\n",
    "        xg = xgb.XGBClassifier(use_label_encoder=False, eval_metric='error', **xb_hyper)\n",
    "        xg.fit(X, y)\n",
    "        xg_importances = pd.DataFrame({'XGB order':df.columns, 'Coef':xg.feature_importances_}).sort_values(by='Coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Feature selection from Recursive Feature Elimination\n",
    "        estimator = xgb.XGBClassifier(use_label_encoder=False, eval_metric='error', **xb_hyper)\n",
    "        selector = RFE(estimator, n_features_to_select=X.shape[1], step=1)\n",
    "        selector = selector.fit(X, y)\n",
    "        rfe_support = selector.get_support()\n",
    "        rfe_importances = pd.DataFrame({'RFE order': df.columns, 'Coef': range(len(df.columns))})\n",
    "        rfe_importances['Coef'] = np.where(rfe_importances['RFE order'].isin(df.columns[rfe_support]),\n",
    "                                                     rfe_importances['Coef'], np.nan)\n",
    "\n",
    "        # Mutual Information\n",
    "        mi_scores = []\n",
    "        for feature in df.columns:\n",
    "            mi = mutual_info_score(df[feature], y)\n",
    "            mi_scores.append(mi)\n",
    "        mi_df = pd.DataFrame({'MI order': df.columns, 'Coef': mi_scores}).sort_values(by='Coef', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Create a dataframe for the current target column, add 'target_col' column\n",
    "        ranks = pd.concat([lasso_ranks, rf_importances, xg_importances, rfe_importances, mi_df], axis=1)\n",
    "        ranks['target_col'] = target_col\n",
    "\n",
    "        total_ranking = pd.concat([total_ranking, ranks])\n",
    "        \n",
    "\n",
    "\n",
    "    total_ranking.reset_index(drop=True, inplace=True)\n",
    "    return total_ranking\n",
    "\n",
    "\n",
    "def voting(df):\n",
    "    unique_targets = df['target_col'].unique()\n",
    "    all_scores = {}\n",
    "    for target in unique_targets:\n",
    "        final_scores = {}\n",
    "        rows = df[df['target_col'] == target]\n",
    "        lasso = rows['Lasso order'].tolist()\n",
    "        rf = rows['RF order'].tolist()\n",
    "        xgb = rows['XGB order'].tolist()\n",
    "        rfe = rows['RFE order'].tolist()\n",
    "        mi = rows['MI order'].tolist()\n",
    "        w1, w2, w3, w4, w5 = 0.3, 0.2, 0.2, 0.1, 0.1\n",
    "        # Lists and weights combined for convenience\n",
    "        lists_and_weights = [(lasso, w1), (rf, w2), (xgb, w3), (rfe, w4), (mi, w5)]\n",
    "        # Calculate final scores\n",
    "        for feature_list, weight in lists_and_weights:\n",
    "            for i, feature in enumerate(reversed(feature_list)):\n",
    "                if feature not in final_scores:\n",
    "                    final_scores[feature] = 0\n",
    "                final_scores[feature] += (i+1) * weight\n",
    "\n",
    "        # Sort features by final scores in descending order\n",
    "        final_ranking = sorted(final_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        all_scores[target] = final_ranking\n",
    "        \n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def get_data(path, columns):\n",
    "    df = pd.read_csv(path)\n",
    "    labels = ['Class', '1st']\n",
    "    y = df[labels]\n",
    "    X = df[columns]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "max_depth = [int(x) for x in np.linspace(1, 1000, num = 10)] #Max depth list for trees\n",
    "max_depth.append(None)\n",
    "\n",
    "model_params = {\n",
    "    'XGBoost':{\n",
    "        'model':XGBClassifier(use_label_encoder=False, eval_metric='error'),\n",
    "        'params' : {\n",
    "            'max_depth': [int(x) for x in np.linspace(1, 50, num = 10)],\n",
    "            'alpha': [int(x) for x in np.linspace(0.1, 10, num = 100)],\n",
    "            'learning_rate': [int(x) for x in np.linspace(0.1, 5, num = 10)],\n",
    "            'n_estimators':[int(x) for x in np.linspace(start = 5, stop = 1000, num = 10)]\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [int(x) for x in np.linspace(start = 5, stop = 1000, num = 10)],\n",
    "            'max_features': ['log2'],\n",
    "            'max_depth': max_depth\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y = get_data(data_dict[ethnic])\n",
    "hypers = hyper_param_search(X, y, 5, model_params, 50)\n",
    "xb_scores = hypers[hypers['model'] == 'XGBoost'].reset_index(drop=True)\n",
    "rf_scores = hypers[hypers['model'] == 'random_forest'].reset_index(drop=True)\n",
    "ranks = ranking(X, y, rf_scores, xb_scores)\n",
    "scoring = voting(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in list(scoring.keys()):\n",
    "    # Split the features and scores\n",
    "    features, scores = zip(*scoring[key])\n",
    "    # Limit to top 100 features\n",
    "    features = features[:100]\n",
    "    scores = scores[:100]\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots(figsize=(10, 15))  # Increase the size of your figure\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    ax.barh(features, scores, color='blue', alpha=0.6)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    label_opts = {'color': 'black', 'bbox': dict(facecolor='white', edgecolor='none')}\n",
    "    ax.set_xlabel('Score', **label_opts)\n",
    "    ax.set_ylabel('Feature', **label_opts)\n",
    "    ax.set_title(f'EC {key} ranking model {type}')\n",
    "\n",
    "    ax.tick_params(axis='both', which='both', labelsize=8, labelcolor='black', colors='black')  # Reduce font size\n",
    "\n",
    "    plt.yticks(rotation=45)  # Rotate y-axis labels\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'ec_ranking_{key}_{type}.png', bbox_inches='tight', transparent=False, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Correlation analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}